{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "Illustration of how to implement a Linear Regeression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Imports\n",
    "\n",
    "import torch\n",
    "from torch import nn as nn\n",
    "import numpy as np\n",
    "from torch import tensor, save\n",
    "import torch.optim as optim\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim):\n",
    "        \n",
    "        \"\"\"\n",
    "        Initializer for the Model. You should instantiate your layers here.\n",
    "        This may require you to feed the input and the output dimensions that you require.\n",
    "        @param input_dim: The dimension of each sample of the input\n",
    "        \"\"\"\n",
    "        \n",
    "        super(LinearRegression, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        # The output dimension of the linear layer is 1 since we need to predict 1 continuous value for each sample\n",
    "        \n",
    "        self.linear = nn.Linear(self.input_dim, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        \"\"\"\n",
    "        In this function, you use the input x and transform it using the layers specified in the\n",
    "        __init__() function.\n",
    "        \"\"\"\n",
    "        \n",
    "        linear_transform = self.linear(x)\n",
    "        \n",
    "        return linear_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_linear_regression():\n",
    "    \n",
    "    \"\"\"\n",
    "    Function that trains the linear model\n",
    "    \"\"\"\n",
    "\n",
    "    # Generating a random tensor with 32 samples each of which is represented through 8 dimensions\n",
    "    inputs = torch.rand(32, 8)\n",
    "    \n",
    "    # Generating the output tensor for illustration.\n",
    "    labels = torch.rand(32, 1)\n",
    "    \n",
    "    # Initializing training specific variables\n",
    "    model = LinearRegression(input_dim = 8)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Optimizer to change the weights of the model. Will be explained in next week's class.\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    epochs = 10\n",
    "    for ep in range(epochs):\n",
    "        # In this example, we will use the same input tensor each time, but ideally you should be dividing your\n",
    "        # data in small minibatches and train one batch in one iteration\n",
    "        for i in range(0, 100):\n",
    "\n",
    "            # Set the state of the model to 'train'. This is important for backpropagation step about which you\n",
    "            # learn in the next class. Backpropagation requires the values computed in the forward() function for\n",
    "            # each layer. Invoking the .train() function directs the model to save these values for future\n",
    "            # backpropagation step.\n",
    "            model.train()\n",
    "            \n",
    "            # Very important to reset the gradients to zero before training a minibatch. Otherwise it will keep \n",
    "            # adding to gradients computed in the previous iterations\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # print (inputs, labels)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Perform the backpropagation step to compute gradients\n",
    "            loss.backward()\n",
    "            # Perform the gradient update\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        print('Epoch %d, loss:%.4f' % (ep+1, running_loss/100))\n",
    "        running_loss = 0.\n",
    "    \n",
    "    print('Model State ===>')\n",
    "    print(model.state_dict())\n",
    "    \n",
    "    # Saving the trained model\n",
    "    save({'epochs': 2, 'model_state_dict': model.state_dict()}, 'linear_regression.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_linear_regression():\n",
    "\n",
    "    # Generating a random tensor with 32 samples each of which is represented through 8 dimensions\n",
    "    inputs = torch.rand(32, 8)\n",
    "    \n",
    "    # Initializing an object of LinearRegression class\n",
    "    model = LinearRegression(input_dim = 8)\n",
    "    \n",
    "    # Load the checkpoint stored from the file\n",
    "    checkpoint = torch.load('linear_regression.ckpt')\n",
    "    \n",
    "    # Load the Model state\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        \n",
    "    # Set the flag to .eval() since we now don't need to store the values computed in the forward(). This is\n",
    "    # at the prediction time we don't need to perform a backpropagation step.\n",
    "    model.eval()\n",
    "    \n",
    "    outputs = model(inputs)\n",
    "    \n",
    "    # Ignore the detach() for now. We'll discuss that in next section\n",
    "    return outputs.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Linear Regression Model\n",
      "\n",
      "Epoch 1, loss:0.1311\n",
      "Epoch 2, loss:0.0536\n",
      "Epoch 3, loss:0.0530\n",
      "Epoch 4, loss:0.0529\n",
      "Epoch 5, loss:0.0528\n",
      "Epoch 6, loss:0.0528\n",
      "Epoch 7, loss:0.0528\n",
      "Epoch 8, loss:0.0528\n",
      "Epoch 9, loss:0.0528\n",
      "Epoch 10, loss:0.0528\n",
      "Model State ===>\n",
      "OrderedDict([('linear.weight', tensor([[ 0.2297,  0.0362,  0.4637,  0.0807,  0.1550, -0.1666,  0.2378,  0.2464]])), ('linear.bias', tensor([-0.0522]))])\n",
      "\n",
      "Predicting using Linear Regression Model\n",
      "[[0.65844446]\n",
      " [0.7625149 ]\n",
      " [0.8820373 ]\n",
      " [0.40817782]\n",
      " [0.539098  ]\n",
      " [0.49190396]\n",
      " [0.5486369 ]\n",
      " [0.3294576 ]\n",
      " [0.26806343]\n",
      " [0.9181377 ]\n",
      " [0.5740857 ]\n",
      " [0.40673345]\n",
      " [0.73572147]\n",
      " [0.42749745]\n",
      " [0.5737763 ]\n",
      " [0.5658042 ]\n",
      " [0.5307116 ]\n",
      " [0.72412723]\n",
      " [0.6921247 ]\n",
      " [0.70571715]\n",
      " [0.5919994 ]\n",
      " [0.32175344]\n",
      " [0.47406918]\n",
      " [0.73307294]\n",
      " [0.64957833]\n",
      " [0.6208963 ]\n",
      " [0.5455258 ]\n",
      " [0.95511454]\n",
      " [0.5975401 ]\n",
      " [0.60348624]\n",
      " [0.87858635]\n",
      " [0.79118305]]\n"
     ]
    }
   ],
   "source": [
    "# Call the training function followed by the prediction function\n",
    "print('Training Linear Regression Model\\n')\n",
    "train_linear_regression()\n",
    "\n",
    "print('\\nPredicting using Linear Regression Model')\n",
    "outputs = predict_linear_regression()\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Classifier\n",
    "\n",
    "Illustration of how to implement a Logistic Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim):\n",
    "        \n",
    "        \"\"\"\n",
    "        Initializer for the Model. You should instantiate your layers here.\n",
    "        This may require you to feed the input and the output dimensions that you require.\n",
    "        @param input_dim: The dimension of each sample of the input\n",
    "        \"\"\"\n",
    "        \n",
    "        super(LogisticClassifier , self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        # The output dimension of the linear layer is 1 since we are using a sigmoid layer to\n",
    "        # to perform binary classification\n",
    "        \n",
    "        self.linear = nn.Linear(self.input_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        \"\"\"\n",
    "        In this function, you use the input x and transform it using the layers specified in the\n",
    "        __init__() function.\n",
    "        \"\"\"\n",
    "        \n",
    "        linear_transform = self.linear(x)\n",
    "        sigmoid = self.sigmoid(linear_transform)\n",
    "        \n",
    "        return linear_transform, sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_logistic_classifier():\n",
    "    \n",
    "    \"\"\"\n",
    "    Function that trains the linear model\n",
    "    \"\"\"\n",
    "\n",
    "    # Generating a random tensor with 32 samples each of which is represented through 8 dimensions\n",
    "    inputs = torch.rand(32, 8)\n",
    "    \n",
    "    # Generating the output tensor for illustration.\n",
    "    labels = torch.randint(low=0, high=2, size=(32, 1))\n",
    "    \n",
    "    # Initializing training specific variables\n",
    "    model = LogisticClassifier(input_dim = 8)\n",
    "    \n",
    "    # Refer http://www.philkr.net/cs342/doc/sigmoid/ for more discussion on this loss function.\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    # Optimizer to change the weights of the model. Will be explained in next week's class.\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    epochs = 10\n",
    "    for ep in range(epochs):\n",
    "        # In this example, we will use the same input tensor each time, but ideally you should be dividing your\n",
    "        # data in small minibatches and train one batch in one iteration\n",
    "        for i in range(0, 100):\n",
    "\n",
    "            # Set the state of the model to 'train'. This is important for backpropagation step about which you\n",
    "            # learn in the next class. Backpropagation requires the values computed in the forward() function for\n",
    "            # each layer. Invoking the .train() function directs the model to save these values for future\n",
    "            # backpropagation step.\n",
    "            # Refer https://pytorch.org/docs/stable/nn.html#torch.nn.Module.train\n",
    "            model.train()\n",
    "            \n",
    "            # Very important to reset the gradients to zero before training a minibatch. Otherwise it will keep \n",
    "            # adding to gradients computed in the previous iterations\n",
    "            # Refer https://pytorch.org/docs/stable/nn.html#torch.nn.Module.zero_grad\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # print (inputs, labels)\n",
    "\n",
    "            # During training, since we are using the BCEWithLogitsLoss which is a combination of a sigmoid layer\n",
    "            # and binary cross entropy, hence, we don't need to use the output of the sigmoid layer from the model.\n",
    "            # Thus, just ignore for this step.\n",
    "            \n",
    "            outputs, _ = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Perform the backpropagation step to compute gradients\n",
    "            loss.backward()\n",
    "            # Perform the gradient update\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        print('Epoch %d, loss:%.4f' % (ep+1, running_loss/100))\n",
    "        running_loss = 0.\n",
    "    \n",
    "    print('Model State ===>')\n",
    "    print(model.state_dict())\n",
    "    \n",
    "    # Saving the trained model\n",
    "    save({'epochs': 2, 'model_state_dict': model.state_dict()}, 'logistic_classifier.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_binary_class():\n",
    "\n",
    "    # Generating a random tensor with 32 samples each of which is represented through 8 dimensions\n",
    "    inputs = torch.rand(32, 8)\n",
    "    \n",
    "    # Initializing an object of LogisticClassifier Class\n",
    "    model = LogisticClassifier(input_dim = 8)\n",
    "    \n",
    "    # Load the checkpoint stored from the file\n",
    "    checkpoint = torch.load('logistic_classifier.ckpt')\n",
    "    \n",
    "    # Load the Model state\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        \n",
    "    # Set the flag to .eval() since we now don't need to store the values computed in the forward(). This is\n",
    "    # at the prediction time we don't need to perform a backpropagation step.\n",
    "    # Refer https://pytorch.org/docs/stable/nn.html#torch.nn.Module.eval\n",
    "    model.eval()\n",
    "    \n",
    "    # We don't need the output of the linear transformation for the prediction. So we just ignore that\n",
    "    _, outputs = model(inputs)\n",
    "    \n",
    "    # Perform the boundary test to classify as 0 or 1\n",
    "    outputs = outputs >= 0.5\n",
    "    \n",
    "    # Ignore the detach() for now. We'll discuss that in next section\n",
    "    return outputs.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Logistic Classifier Model\n",
      "\n",
      "Epoch 1, loss:0.6511\n",
      "Epoch 2, loss:0.6168\n",
      "Epoch 3, loss:0.5962\n",
      "Epoch 4, loss:0.5806\n",
      "Epoch 5, loss:0.5684\n",
      "Epoch 6, loss:0.5588\n",
      "Epoch 7, loss:0.5509\n",
      "Epoch 8, loss:0.5444\n",
      "Epoch 9, loss:0.5389\n",
      "Epoch 10, loss:0.5343\n",
      "Model State ===>\n",
      "OrderedDict([('linear.weight', tensor([[ 1.0220, -1.7958, -1.0330, -1.1648, -1.8942,  0.0168,  1.0348,  0.8753]])), ('linear.bias', tensor([0.8121]))])\n",
      "\n",
      "Prediciting using Logisitc Classifier\n",
      "\n",
      "[[1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "# Call the training function followed by the prediction function\n",
    "print('Training Logistic Classifier Model\\n') \n",
    "train_logistic_classifier()\n",
    "\n",
    "print('\\nPrediciting using Logisitc Classifier\\n')\n",
    "outputs = predict_binary_class()\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
